{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'TEXT: (.+)'\n",
    "with open('collections/sample.txt', 'r') as file:\n",
    "    txt = file.read()\n",
    "    documents = re.findall(pattern, txt)\n",
    "\n",
    "# print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like', 'wink', 'like', 'drink']\n",
      "['like', 'drink', 'drink', 'drink']\n",
      "['thing', 'like', 'drink', 'ink']\n",
      "['ink', 'like', 'drink', 'pink']\n",
      "['like', 'wink', 'drink', 'pink', 'ink']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stemming.porter2 import stem \n",
    "\n",
    "tokenization = RegexpTokenizer(r\"\\w+\")\n",
    "new = []\n",
    "\n",
    "with open('stopWords.txt', 'r') as file:\n",
    "    stopWords = file.read().split()\n",
    "\n",
    "def preprocess(stp, stm, documents):\n",
    "    for doc in documents:\n",
    "        # Tokenization\n",
    "        tokenized = tokenization.tokenize(doc)\n",
    "        \n",
    "        # Casefolding\n",
    "        lower = [word.lower() for word in tokenized]\n",
    "        \n",
    "        if stp:\n",
    "            # Stopwords removal\n",
    "            lower = [word for word in lower if not word in stopWords]\n",
    "        \n",
    "        if stm:\n",
    "            # Stemming using the stemming library\n",
    "            lower = [stem(word) for word in lower]\n",
    "        \n",
    "        new.append(lower)\n",
    "\n",
    "preprocess(True, True, documents)\n",
    "# print(new)\n",
    "for list in new:\n",
    "    print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['like wink like drink', 'like drink drink drink', 'thing like drink ink', 'ink like drink pink', 'like wink drink pink ink']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate tokens\n",
    "final=[]\n",
    "for i in new:\n",
    "    final.append(' '.join(i))\n",
    "print(final)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': {0: [0, 2], 1: [0], 2: [1], 3: [1], 4: [0]}, 'wink': {0: [1], 4: [1]}, 'drink': {0: [3], 1: [1, 2, 3], 2: [2], 3: [2], 4: [2]}, 'thing': {2: [0]}, 'ink': {2: [3], 3: [0], 4: [4]}, 'pink': {3: [3], 4: [3]}}\n"
     ]
    }
   ],
   "source": [
    "def positional_inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for pos, word in enumerate(doc.split()):\n",
    "            if word not in index:\n",
    "                index[word] = {}\n",
    "            if doc_id not in index[word]:\n",
    "                index[word][doc_id] = []\n",
    "            index[word][doc_id].append(pos)\n",
    "    return index\n",
    "\n",
    "pos_index = positional_inverted_index(final)\n",
    "print(pos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for pos, word in enumerate(doc.split()):\n",
    "            if word not in index:\n",
    "                index[word] = {\n",
    "                    'document_frequency': 0,\n",
    "                    'documents': {},\n",
    "                }\n",
    "            if doc_id not in index[word]['documents']:\n",
    "                index[word]['documents'][doc_id] = []\n",
    "                index[word]['document_frequency'] += 1\n",
    "            index[word]['documents'][doc_id].append(pos)\n",
    "    return index\n",
    "\n",
    "pos_index = positional_inverted_index(final)\n",
    "# print(pos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like:\n",
      "\t1: 0, 2\n",
      "\t2: 0\n",
      "\t3: 1\n",
      "\t4: 1\n",
      "\t5: 0\n",
      "wink:\n",
      "\t1: 1\n",
      "\t5: 1\n",
      "drink:\n",
      "\t1: 3\n",
      "\t2: 1, 2, 3\n",
      "\t3: 2\n",
      "\t4: 2\n",
      "\t5: 2\n",
      "thing:\n",
      "\t3: 0\n",
      "ink:\n",
      "\t3: 3\n",
      "\t4: 0\n",
      "\t5: 4\n",
      "pink:\n",
      "\t4: 3\n",
      "\t5: 3\n"
     ]
    }
   ],
   "source": [
    "for term, info in pos_index.items():\n",
    "    print(term + \":\")\n",
    "    for doc_id, positions in info['documents'].items():\n",
    "        print(f\"\\t{doc_id+1}: {', '.join(map(str, positions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term\tDocument ID\tPositions\n",
      "like:\n",
      "\t1:\t\t0, 2\n",
      "\t2:\t\t0\n",
      "\t3:\t\t1\n",
      "\t4:\t\t1\n",
      "\t5:\t\t0\n",
      "wink:\n",
      "\t1:\t\t1\n",
      "\t5:\t\t1\n",
      "drink:\n",
      "\t1:\t\t3\n",
      "\t2:\t\t1, 2, 3\n",
      "\t3:\t\t2\n",
      "\t4:\t\t2\n",
      "\t5:\t\t2\n",
      "thing:\n",
      "\t3:\t\t0\n",
      "ink:\n",
      "\t3:\t\t3\n",
      "\t4:\t\t0\n",
      "\t5:\t\t4\n",
      "pink:\n",
      "\t4:\t\t3\n",
      "\t5:\t\t3\n"
     ]
    }
   ],
   "source": [
    "print(\"Term\\tDocument ID\\tPositions\")\n",
    "for term, info in pos_index.items():\n",
    "    print(term + \":\")\n",
    "    for doc_id, positions in info['documents'].items():\n",
    "        print(f\"\\t{doc_id+1}:\\t\\t{', '.join(map(str, positions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sampleindex.txt', 'w') as file:\n",
    "    file.write(\"Term\\tDocument ID\\tPositions\\n\")\n",
    "    for term, info in pos_index.items():\n",
    "        file.write(term + \":\\n\")\n",
    "        for doc_id, positions in info['documents'].items():\n",
    "            file.write(f\"\\t{doc_id}:\\t\\t{', '.join(map(str, positions))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_index = positional_inverted_index(final)\n",
    "\n",
    "# with open('sampleindex.txt', 'w') as file:\n",
    "#     file.write(\"Term\\tDocument ID\\tPositions\\n\")\n",
    "#     for term, info in pos_index.items():\n",
    "#         file.write(term + \":\\n\")\n",
    "#         for doc_id, positions in info['documents'].items():\n",
    "#             file.write(f\"\\t{doc_id}:\\t\\t{', '.join(map(str, positions))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = positional_inverted_index(final)\n",
    "\n",
    "with open('sampleindex.txt', 'w') as file:\n",
    "    for term, info in pos_index.items():\n",
    "        file.write(term + \":\\n\")\n",
    "        for doc_id, positions in info['documents'].items():\n",
    "            file.write(f\"\\t{doc_id}:\\t{', '.join(map(str, positions))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pos_index = positional_inverted_index(final)\n",
    "\n",
    "# Save the inverted index to a binary file\n",
    "with open('sampleindex.pkl', 'wb') as file:\n",
    "    pickle.dump(pos_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the inverted index from the binary file\n",
    "with open('sampleindex.pkl', 'rb') as file:\n",
    "    pos_indexs = pickle.load(file)\n",
    "\n",
    "# print(pos_indexs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': [0, 1, 2, 3, 4], 'wink': [0, 4], 'drink': [0, 1, 2, 3, 4], 'thing': [2], 'ink': [2, 3, 4], 'pink': [3, 4]}\n"
     ]
    }
   ],
   "source": [
    "# inverted index\n",
    "def inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for word in doc.split():\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            if doc_id not in index[word]:\n",
    "                index[word].append(doc_id)\n",
    "    return index\n",
    "\n",
    "inv_index = inverted_index(final)\n",
    "print(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_postings(posting1, posting2):\n",
    "    p1 = 0\n",
    "    p2 = 0\n",
    "    result = list()\n",
    "    while p1 < len(posting1) and p2 < len(posting2):\n",
    "        if posting1[p1] == posting2[p2]:\n",
    "            result.append(posting1[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif posting1[p1] > posting2[p2]:\n",
    "            p2 += 1\n",
    "        else:\n",
    "            p1 += 1\n",
    "    return result\n",
    "\n",
    "def or_postings(posting1, posting2):\n",
    "    return sorted(set(posting1 + posting2))\n",
    "\n",
    "def NOT(posting, total_docs):\n",
    "    return sorted(set(range(total_docs)) - set(posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hi'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEnter your query - \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(process_query(query))\n",
      "\u001b[1;32m/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb Cell 19\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m         result \u001b[39m=\u001b[39m NOT(inv_index[query_terms[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39melif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         result \u001b[39m=\u001b[39m inv_index[term]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X24sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hi'"
     ]
    }
   ],
   "source": [
    "# ... (import statements and preprocessing code)\n",
    "\n",
    "def inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for word in doc.split():\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            if doc_id not in index[word]:\n",
    "                index[word].append(doc_id)\n",
    "    return index\n",
    "\n",
    "def positional_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for pos, word in enumerate(doc.split()):\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            index[word].append((doc_id, pos))\n",
    "    return index\n",
    "\n",
    "inv_index = inverted_index(final)\n",
    "pos_index = positional_index(final)\n",
    "\n",
    "# ... (code for or_postings, NOT, and and_postings functions)\n",
    "\n",
    "def process_query(query):\n",
    "    query_terms = query.split()\n",
    "    result = None\n",
    "    for i, term in enumerate(query_terms):\n",
    "        if term == \"AND\":\n",
    "            if query_terms[i+1] == \"NOT\":\n",
    "                result = and_postings(result, NOT(inv_index[query_terms[i+2]]))\n",
    "            else:\n",
    "                result = and_postings(result, inv_index[query_terms[i+1]])\n",
    "        elif term == \"OR\":\n",
    "            if query_terms[i+1] == \"NOT\":\n",
    "                result = or_postings(result, NOT(inv_index[query_terms[i+2]]))\n",
    "            else:\n",
    "                result = or_postings(result, inv_index[query_terms[i+1]])\n",
    "        elif term == \"NOT\" and i == 0:\n",
    "            result = NOT(inv_index[query_terms[i+1]])\n",
    "        elif i == 0:\n",
    "            result = inv_index[term]\n",
    "    return result\n",
    "\n",
    "query = input('Enter your query - ')\n",
    "print(process_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEnter your query - \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39mprint\u001b[39m(process_query(query))\n",
      "\u001b[1;32m/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb Cell 21\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         result \u001b[39m=\u001b[39m NOT(inv_index[query_terms[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]], total_docs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39melif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m         result \u001b[39m=\u001b[39m inv_index[term]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Github/M2/S1/RI/Lab_02/Lab_02.ipynb#X26sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hello'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "pattern = r'TEXT: (.+)'\n",
    "with open('collections/sample.txt', 'r') as file:\n",
    "    txt = file.read()\n",
    "    documents = re.findall(pattern, txt)\n",
    "\n",
    "tokenization = RegexpTokenizer(r\"\\w+\")\n",
    "new = []\n",
    "\n",
    "with open('stopWords.txt', 'r') as file:\n",
    "    stopWords = file.read().split()\n",
    "\n",
    "def preprocess(stp, stm, documents):\n",
    "    for doc in documents:\n",
    "        tokenized = tokenization.tokenize(doc)\n",
    "        lower = [word.lower() for word in tokenized]\n",
    "        if stp:\n",
    "            lower = [word for word in lower if not word in stopWords]\n",
    "        if stm:\n",
    "            lower = [PorterStemmer().stem(word) for word in lower]\n",
    "        new.append(lower)\n",
    "\n",
    "preprocess(False, False, documents)\n",
    "\n",
    "final = [' '.join(i) for i in new]\n",
    "\n",
    "def inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for word in doc.split():\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            if doc_id not in index[word]:\n",
    "                index[word].append(doc_id)\n",
    "    return index\n",
    "\n",
    "inv_index = inverted_index(final)\n",
    "\n",
    "def and_postings(posting1, posting2):\n",
    "    return sorted(set(posting1).intersection(posting2))\n",
    "\n",
    "def or_postings(posting1, posting2):\n",
    "    return sorted(set(posting1).union(posting2))\n",
    "\n",
    "def NOT(posting, total_docs):\n",
    "    return sorted(set(range(total_docs)) - set(posting))\n",
    "\n",
    "def process_query(query):\n",
    "    query_terms = query.split()\n",
    "    result = None\n",
    "    total_docs = len(final)\n",
    "    for i, term in enumerate(query_terms):\n",
    "        if term == \"AND\":\n",
    "            if query_terms[i+1] == \"NOT\":\n",
    "                result = and_postings(result, NOT(inv_index[query_terms[i+2]], total_docs))\n",
    "            else:\n",
    "                result = and_postings(result, inv_index[query_terms[i+1]])\n",
    "        elif term == \"OR\":\n",
    "            if query_terms[i+1] == \"NOT\":\n",
    "                result = or_postings(result, NOT(inv_index[query_terms[i+2]], total_docs))\n",
    "            else:\n",
    "                result = or_postings(result, inv_index[query_terms[i+1]])\n",
    "        elif term == \"NOT\" and i == 0:\n",
    "            result = NOT(inv_index[query_terms[i+1]], total_docs)\n",
    "        elif i == 0:\n",
    "            result = inv_index[term]\n",
    "    return result\n",
    "\n",
    "query = input('Enter your query - ')\n",
    "print(process_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
