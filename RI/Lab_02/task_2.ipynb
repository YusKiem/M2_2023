{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stemming.porter2 import stem \n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "pattern = r'TEXT: (.+)'\n",
    "with open('collections/sample.txt', 'r') as file:\n",
    "    x = file.read()\n",
    "    documents = re.findall(pattern, x)\n",
    "\n",
    "\n",
    "# print(documents)\n",
    "\n",
    "tokenization = RegexpTokenizer(r\"\\w+\")\n",
    "new = []\n",
    "\n",
    "with open('stopWords.txt', 'r') as file:\n",
    "    stopWords = file.read().split()\n",
    "\n",
    "def preprocess(stp, stm, documents):\n",
    "    for doc in documents:\n",
    "        # Tokenization\n",
    "        tokenized = tokenization.tokenize(doc)\n",
    "        \n",
    "        # Casefolding\n",
    "        lower = [word.lower() for word in tokenized]\n",
    "\n",
    "        if stp:\n",
    "            # Stopwords removal\n",
    "            lower = [word for word in lower if not word in stopWords]\n",
    "        \n",
    "        if stm:\n",
    "            # Stemming using the stemming library\n",
    "            lower = [stem(word) for word in lower]\n",
    "        \n",
    "        new.append(lower)\n",
    "\n",
    "preprocess(False, False, documents)\n",
    "# print(new)\n",
    "for list in new:\n",
    "    print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate tokens\n",
    "# final=[]\n",
    "# for i in new:\n",
    "#     final.append(' '.join(i))\n",
    "# print(final)\n",
    "final = [' '.join(i) for i in new] \n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for pos, word in enumerate(doc.split()):\n",
    "            if word not in index:\n",
    "                index[word] = {\n",
    "                    'document_frequency': 0,\n",
    "                    'documents': {},\n",
    "                }\n",
    "            if doc_id not in index[word]['documents']:\n",
    "                index[word]['documents'][doc_id] = []\n",
    "                index[word]['document_frequency'] += 1\n",
    "            index[word]['documents'][doc_id].append(pos)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = positional_inverted_index(final)\n",
    "with open('sampleindex.txt', 'w') as file:\n",
    "    for term, info in pos_index.items():\n",
    "        file.write(term + \":\\n\")\n",
    "        for doc_id, positions in info['documents'].items():\n",
    "            file.write(f\"\\t{doc_id}:\\t{', '.join(map(str, positions))}\\n\")\n",
    "\n",
    "print(\"Term\\tDocument ID\\tPositions\")\n",
    "for term, info in pos_index.items():\n",
    "    print(term + \":\")\n",
    "    for doc_id, positions in info['documents'].items():\n",
    "        print(f\"\\t{doc_id+1}: {', '.join(map(str, positions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for word in doc.split():\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            if doc_id not in index[word]:\n",
    "                index[word].append(doc_id)\n",
    "    return index\n",
    "\n",
    "inv_index = inverted_index(final)\n",
    "inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_postings(posting1, posting2):\n",
    "    return sorted(set(posting1).intersection(posting2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_postings(posting1, posting2):\n",
    "    return sorted(set(posting1).union(posting2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT(posting, total_docs):\n",
    "    return sorted(set(range(total_docs)) - set(posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Search\n",
    "def boolean_search(query, index):\n",
    "    query = query.split()\n",
    "    result = set(index[query[0]].keys())\n",
    "    for term in query[1:]:\n",
    "        result &= set(index[term].keys())\n",
    "    return result\n",
    "\n",
    "query = \"he AND NOT ice\"\n",
    "result = boolean_search(query, inv_index)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, inv_index, total_docs):\n",
    "    query_terms = query.split()\n",
    "    result = None\n",
    "\n",
    "    for i, term in enumerate(query_terms):\n",
    "        if term == \"AND\" or term == \"OR\":\n",
    "            if query_terms[i+1] == \"NOT\":\n",
    "                operand = NOT(inv_index[query_terms[i+2]], total_docs)\n",
    "            else:\n",
    "                operand = inv_index[query_terms[i+1]]\n",
    "\n",
    "            if term == \"AND\":\n",
    "                result = and_postings(result, operand)\n",
    "            elif term == \"OR\":\n",
    "                result = or_postings(result, operand)\n",
    "\n",
    "        elif term == \"NOT\" and i == 0:\n",
    "            result = NOT(inv_index[query_terms[i+1]], total_docs)\n",
    "        elif i == 0:\n",
    "            try:\n",
    "                result = inv_index[term]\n",
    "            except KeyError:\n",
    "                result = set()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "total_docs = len(final)\n",
    "query = input('Enter your query - ')\n",
    "result = process_query(query, inv_index, total_docs)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Index\n",
    "with open('index.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_index, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Index\n",
    "with open('index.pkl', 'rb') as f:\n",
    "    index_loaded = pickle.load(f)\n",
    "# print(index_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    query_terms = query.split()\n",
    "    i = 0\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    while i < len(query_terms):\n",
    "        term = query_terms[i]\n",
    "\n",
    "        if term == 'AND':\n",
    "            result = and_postings(inv_index[query_terms[i-1]], inv_index[query_terms[i+1]], total_docs)\n",
    "        elif term == 'OR':\n",
    "            result = or_postings(inv_index[query_terms[i-1]], inv_index[query_terms[i+1]], total_docs)\n",
    "        elif term == 'NOT':\n",
    "            result = NOT(inv_index[query_terms[i+1]], total_docs)\n",
    "        elif i == 0:\n",
    "            try:\n",
    "                result = inv_index[term]\n",
    "            except KeyError:\n",
    "                result = set()\n",
    "        i += 2  # Move to the next query term\n",
    "\n",
    "    return result\n",
    "\n",
    "query = input('Enter your query - ')\n",
    "print(process_query(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_search(query, pos_index):\n",
    "    query_terms = query.split()\n",
    "    postings = pos_index[query_terms[0]]['documents']\n",
    "    for term in query_terms[1:]:\n",
    "        term_postings = pos_index[term]['documents']\n",
    "        for doc in postings.keys():\n",
    "            if doc in term_postings:\n",
    "                postings[doc] = [pos for pos in postings[doc] if pos + 1 in term_postings[doc]]\n",
    "            else:\n",
    "                postings.pop(doc)\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_search(query, pos_index):\n",
    "    query_terms = query.split()\n",
    "    result = pos_index[query_terms[0]]['documents'].copy()  # Create a copy of the initial postings\n",
    "    for term in query_terms[1:]:\n",
    "        term_postings = pos_index[term]['documents']\n",
    "        updated_postings = {}\n",
    "        for doc in result.keys():\n",
    "            if doc in term_postings:\n",
    "                updated_postings[doc] = [pos for pos in result[doc] if pos + 1 in term_postings[doc]]\n",
    "        result = updated_postings  # Update the result with the new postings\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   query = \"he ink and\"\n",
    "   result = phrase_search(query, pos_index)\n",
    "   print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_search(query, k, pos_index):\n",
    "    query_terms = query.split()\n",
    "    postings = pos_index[query_terms[0]]['documents']\n",
    "    for term in query_terms[1:]:\n",
    "        term_postings = pos_index[term]['documents']\n",
    "        for doc in postings.keys():\n",
    "            if doc in term_postings:\n",
    "                postings[doc] = [pos for pos in postings[doc] if any(abs(pos - other_pos) <= k for other_pos in term_postings[doc])]\n",
    "            else:\n",
    "                postings.pop(doc)\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_search(query, k, pos_index):\n",
    "    query_terms = query.split()\n",
    "    result = pos_index[query_terms[0]]['documents'].copy()  # Create a copy of the initial postings\n",
    "    for term in query_terms[1:]:\n",
    "        term_postings = pos_index[term]['documents']\n",
    "        updated_postings = {}\n",
    "        for doc in result.keys():\n",
    "            if doc in term_postings:\n",
    "                updated_postings[doc] = [pos for pos in result[doc] if any(abs(pos - other_pos) <= k for other_pos in term_postings[doc])]\n",
    "        result = updated_postings  # Update the result with the new postings\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   query = \"he ink and\"\n",
    "   k = 5  # Set the proximity threshold\n",
    "   result = proximity_search(query, k, pos_index)\n",
    "   print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Large index: If the index is very large, it would be best to save the index in a database or use an indexing service like Elasticsearch. To load the index into memory, you can load it in chunks or use a memory-mapped file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Large number of documents: If the number of documents is very large, you can use delta encoding to save document numbers. Delta encoding saves the difference between two consecutive numbers instead of the numbers themselves, which can save a lot of space when the numbers are close together."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
