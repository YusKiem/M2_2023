{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stemming.porter2 import stem \n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'likes', 'to', 'wink', 'he', 'likes', 'to', 'drink']\n",
      "['he', 'likes', 'to', 'drink', 'and', 'drink', 'and', 'drink']\n",
      "['the', 'thing', 'he', 'likes', 'to', 'drink', 'is', 'ink']\n",
      "['the', 'ink', 'he', 'likes', 'to', 'drink', 'is', 'pink']\n",
      "['he', 'likes', 'to', 'wink', 'and', 'drink', 'pink', 'ink']\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "pattern = r'TEXT: (.+)'\n",
    "with open('collections/sample.txt', 'r') as file:\n",
    "    x = file.read()\n",
    "    documents = re.findall(pattern, x)\n",
    "\n",
    "\n",
    "# print(documents)\n",
    "\n",
    "tokenization = RegexpTokenizer(r\"\\w+\")\n",
    "new = []\n",
    "\n",
    "with open('stopWords.txt', 'r') as file:\n",
    "    stopWords = file.read().split()\n",
    "\n",
    "def preprocess(stp, stm, documents):\n",
    "    for doc in documents:\n",
    "        # Tokenization\n",
    "        tokenized = tokenization.tokenize(doc)\n",
    "        \n",
    "        # Casefolding\n",
    "        lower = [word.lower() for word in tokenized]\n",
    "\n",
    "        if stp:\n",
    "            # Stopwords removal\n",
    "            lower = [word for word in lower if not word in stopWords]\n",
    "        \n",
    "        if stm:\n",
    "            # Stemming using the stemming library\n",
    "            lower = [stem(word) for word in lower]\n",
    "        \n",
    "        new.append(lower)\n",
    "\n",
    "preprocess(False, False, documents)\n",
    "# print(new)\n",
    "for list in new:\n",
    "    print(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he likes to wink he likes to drink',\n",
       " 'he likes to drink and drink and drink',\n",
       " 'the thing he likes to drink is ink',\n",
       " 'the ink he likes to drink is pink',\n",
       " 'he likes to wink and drink pink ink']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate tokens\n",
    "# final=[]\n",
    "# for i in new:\n",
    "#     final.append(' '.join(i))\n",
    "# print(final)\n",
    "final = [' '.join(i) for i in new] \n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for pos, word in enumerate(doc.split()):\n",
    "            if word not in index:\n",
    "                index[word] = {\n",
    "                    'document_frequency': 0,\n",
    "                    'documents': {},\n",
    "                }\n",
    "            if doc_id not in index[word]['documents']:\n",
    "                index[word]['documents'][doc_id] = []\n",
    "                index[word]['document_frequency'] += 1\n",
    "            index[word]['documents'][doc_id].append(pos)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term\tDocument ID\tPositions\n",
      "he:\n",
      "\t1: 0, 4\n",
      "\t2: 0\n",
      "\t3: 2\n",
      "\t4: 2\n",
      "\t5: 0\n",
      "likes:\n",
      "\t1: 1, 5\n",
      "\t2: 1\n",
      "\t3: 3\n",
      "\t4: 3\n",
      "\t5: 1\n",
      "to:\n",
      "\t1: 2, 6\n",
      "\t2: 2\n",
      "\t3: 4\n",
      "\t4: 4\n",
      "\t5: 2\n",
      "wink:\n",
      "\t1: 3\n",
      "\t5: 3\n",
      "drink:\n",
      "\t1: 7\n",
      "\t2: 3, 5, 7\n",
      "\t3: 5\n",
      "\t4: 5\n",
      "\t5: 5\n",
      "and:\n",
      "\t2: 4, 6\n",
      "\t5: 4\n",
      "the:\n",
      "\t3: 0\n",
      "\t4: 0\n",
      "thing:\n",
      "\t3: 1\n",
      "is:\n",
      "\t3: 6\n",
      "\t4: 6\n",
      "ink:\n",
      "\t3: 7\n",
      "\t4: 1\n",
      "\t5: 7\n",
      "pink:\n",
      "\t4: 7\n",
      "\t5: 6\n"
     ]
    }
   ],
   "source": [
    "pos_index = positional_inverted_index(final)\n",
    "with open('sampleindex.txt', 'w') as file:\n",
    "    for term, info in pos_index.items():\n",
    "        file.write(term + \":\\n\")\n",
    "        for doc_id, positions in info['documents'].items():\n",
    "            file.write(f\"\\t{doc_id}:\\t{', '.join(map(str, positions))}\\n\")\n",
    "\n",
    "print(\"Term\\tDocument ID\\tPositions\")\n",
    "for term, info in pos_index.items():\n",
    "    print(term + \":\")\n",
    "    for doc_id, positions in info['documents'].items():\n",
    "        print(f\"\\t{doc_id+1}: {', '.join(map(str, positions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'he': [0, 1, 2, 3, 4],\n",
       " 'likes': [0, 1, 2, 3, 4],\n",
       " 'to': [0, 1, 2, 3, 4],\n",
       " 'wink': [0, 4],\n",
       " 'drink': [0, 1, 2, 3, 4],\n",
       " 'and': [1, 4],\n",
       " 'the': [2, 3],\n",
       " 'thing': [2],\n",
       " 'is': [2, 3],\n",
       " 'ink': [2, 3, 4],\n",
       " 'pink': [3, 4]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverted_index(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        for word in doc.split():\n",
    "            if word not in index:\n",
    "                index[word] = []\n",
    "            if doc_id not in index[word]:\n",
    "                index[word].append(doc_id)\n",
    "    return index\n",
    "\n",
    "inv_index = inverted_index(final)\n",
    "inv_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def and_postings(posting1, posting2):\n",
    "    return sorted(set(posting1).intersection(posting2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def or_postings(posting1, posting2):\n",
    "    return sorted(set(posting1).union(posting2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT(posting, total_docs):\n",
    "    return sorted(set(range(total_docs)) - set(posting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Search\n",
    "def boolean_search(query, index):\n",
    "    query = query.split()\n",
    "    result = set(index[query[0]].keys())\n",
    "    for term in query[1:]:\n",
    "        result &= set(index[term].keys())\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_query(query):\n",
    "#     query_terms = query.split()\n",
    "#     result = None\n",
    "#     total_docs = len(final)\n",
    "#     for i, term in enumerate(query_terms):\n",
    "#         if term == \"AND\":\n",
    "#             if query_terms[i+1] == \"NOT\":\n",
    "#                 result = and_postings(result, NOT(inv_index[query_terms[i+2]], total_docs))\n",
    "#             else:\n",
    "#                 result = and_postings(result, inv_index[query_terms[i+1]])\n",
    "#         elif term == \"OR\":\n",
    "#             if query_terms[i+1] == \"NOT\":\n",
    "#                 result = or_postings(result, NOT(inv_index[query_terms[i+2]], total_docs))\n",
    "#             else:\n",
    "#                 result = or_postings(result, inv_index[query_terms[i+1]])\n",
    "#         elif term == \"NOT\" and i == 0:\n",
    "#             result = NOT(inv_index[query_terms[i+1]], total_docs)\n",
    "#         elif i == 0:\n",
    "#             try:\n",
    "#                 result = inv_index[term]\n",
    "#             except KeyError:\n",
    "#                 result = set()\n",
    "#         i += 2  # Move to the next query term\n",
    "#     return result\n",
    "\n",
    "# query = input('Enter your query - ')\n",
    "# print(process_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "def process_query(query, inv_index, total_docs):\n",
    "    query_terms = query.split()\n",
    "    result = None\n",
    "\n",
    "    for i, term in enumerate(query_terms):\n",
    "        if term == \"AND\" or term == \"OR\":\n",
    "            if query_terms[i+1] == \"NOT\":\n",
    "                operand = NOT(inv_index[query_terms[i+2]], total_docs)\n",
    "            else:\n",
    "                operand = inv_index[query_terms[i+1]]\n",
    "\n",
    "            if term == \"AND\":\n",
    "                result = and_postings(result, operand)\n",
    "            elif term == \"OR\":\n",
    "                result = or_postings(result, operand)\n",
    "\n",
    "        elif term == \"NOT\" and i == 0:\n",
    "            result = NOT(inv_index[query_terms[i+1]], total_docs)\n",
    "        elif i == 0:\n",
    "            try:\n",
    "                result = inv_index[term]\n",
    "            except KeyError:\n",
    "                result = set()\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "total_docs = len(final)\n",
    "query = input('Enter your query - ')\n",
    "result = process_query(query, inv_index, total_docs)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Index\n",
    "with open('index.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_index, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Index\n",
    "with open('index.pkl', 'rb') as f:\n",
    "    index_loaded = pickle.load(f)\n",
    "# print(index_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    query_terms = query.split()\n",
    "    i = 0\n",
    "    total_docs = len(documents)\n",
    "    \n",
    "    while i < len(query_terms):\n",
    "        term = query_terms[i]\n",
    "\n",
    "        if term == 'AND':\n",
    "            result = and_postings(inv_index[query_terms[i-1]], inv_index[query_terms[i+1]], total_docs)\n",
    "        elif term == 'OR':\n",
    "            result = or_postings(inv_index[query_terms[i-1]], inv_index[query_terms[i+1]], total_docs)\n",
    "        elif term == 'NOT':\n",
    "            result = NOT(inv_index[query_terms[i+1]], total_docs)\n",
    "        elif i == 0:\n",
    "            try:\n",
    "                result = inv_index[term]\n",
    "            except KeyError:\n",
    "                result = set()\n",
    "        i += 2  # Move to the next query term\n",
    "\n",
    "    return result\n",
    "\n",
    "query = input('Enter your query - ')\n",
    "print(process_query(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# def load_index(file_path):\n",
    "#     with open(file_path, 'rb') as file:\n",
    "#         index = pickle.load(file)\n",
    "#     return index\n",
    "\n",
    "\n",
    "# def phrase_search(index, query):\n",
    "#     query_terms = query.split()\n",
    "#     result = set()\n",
    "\n",
    "#     for term in query_terms:\n",
    "#         if term in index:\n",
    "#             if not result:\n",
    "#                 result.update(set(index[term]))\n",
    "#             else:\n",
    "#                 result.intersection_update(set(index[term]))\n",
    "\n",
    "#     return result\n",
    "\n",
    "# def proximity_search(index, query, proximity):\n",
    "#     query_terms = query.split()\n",
    "#     result = set()\n",
    "\n",
    "#     for term in query_terms:\n",
    "#         if term in index:\n",
    "#             if not result:\n",
    "#                 result.update(set(index[term]))\n",
    "#             else:\n",
    "#                 result.intersection_update(set(index[term]))\n",
    "\n",
    "#     # Check proximity\n",
    "#     result = [doc_id for doc_id in result if check_proximity(index, query_terms, doc_id, proximity)]\n",
    "#     return result\n",
    "\n",
    "# def check_proximity(index, query_terms, doc_id, proximity):\n",
    "#     positions = [index[term][doc_id] for term in query_terms if term in index]\n",
    "#     for positions_set in zip(*positions):\n",
    "#         for i in range(1, len(positions_set)):\n",
    "#             if positions_set[i] - positions_set[i-1] <= proximity:\n",
    "#                 return True\n",
    "#     return False\n",
    "\n",
    "# # Load the index into memory\n",
    "# index_path = 'sampleindex.txt'\n",
    "# loaded_index = load_index(index_loaded)\n",
    "\n",
    "# # Example queries\n",
    "# boolean_query = \"term1 AND term2 OR term3 NOT term4\"\n",
    "# phrase_query = \"phrase1 phrase2\"\n",
    "# proximity_query = \"term1 NEAR/5 term2\"\n",
    "\n",
    "# # Run queries\n",
    "# result_boolean = boolean_search(loaded_index, boolean_query)\n",
    "# result_phrase = phrase_search(loaded_index, phrase_query)\n",
    "# result_proximity = proximity_search(loaded_index, proximity_query, proximity=5)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Boolean Search Result:\", result_boolean)\n",
    "# print(\"Phrase Search Result:\", result_phrase)\n",
    "# print(\"Proximity Search Result:\", result_proximity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
